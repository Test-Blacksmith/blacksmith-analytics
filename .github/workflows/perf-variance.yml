name: Performance Variance Analysis

on:
  workflow_dispatch:

jobs:
  variance-test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        run-id: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
    steps:
      - name: Record start time
        id: start
        run: echo "start=$(date +%s%N)" >> $GITHUB_OUTPUT
      
      - name: Standardized workload
        run: |
          echo "Running standardized CPU workload..."
          time python3 -c "import math; result = sum(math.sqrt(i) for i in range(5000000)); print(f'Result: {result}')"
          
          echo "Running I/O workload..."
          dd if=/dev/zero of=testfile bs=1M count=50 2>&1
          rm testfile
          
          echo "Running memory workload..."
          python3 -c "data = [i**2 for i in range(2000000)]; print(f'Processed {len(data)} items')"
      
      - name: Calculate duration
        run: |
          END=$(date +%s%N)
          START=${{ steps.start.outputs.start }}
          DURATION=$(( (END - START) / 1000000 ))
          
          echo "## Run ${{ matrix.run-id }} Results" >> $GITHUB_STEP_SUMMARY
          echo "- Duration: ${DURATION}ms" >> $GITHUB_STEP_SUMMARY
          echo "- Runner: ubuntu-latest" >> $GITHUB_STEP_SUMMARY
          echo "- Timestamp: $(date -Iseconds)" >> $GITHUB_STEP_SUMMARY
          
          mkdir -p metrics
          echo "{\"run_id\": ${{ matrix.run-id }}, \"duration_ms\": $DURATION, \"timestamp\": \"$(date -Iseconds)\"}" > metrics/run-${{ matrix.run-id }}.json
      
      - name: Upload metrics
        uses: actions/upload-artifact@v4
        with:
          name: perf-metrics-${{ matrix.run-id }}
          path: metrics/run-${{ matrix.run-id }}.json

  summary:
    needs: variance-test
    runs-on: ubuntu-latest
    if: always()
    steps:
      - name: Download all metrics
        uses: actions/download-artifact@v4
        with:
          pattern: perf-metrics-*
          merge-multiple: true
      
      - name: Statistical analysis
        run: |
          echo "## ðŸ“Š Performance Variance Analysis" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          python3 << 'PYTHON'
          import json
          import os
          import statistics
          
          durations = []
          for filename in os.listdir('.'):
              if filename.startswith('run-') and filename.endswith('.json'):
                  with open(filename) as f:
                      data = json.load(f)
                      durations.append(data['duration_ms'])
          
          if len(durations) > 1:
              mean = statistics.mean(durations)
              stdev = statistics.stdev(durations)
              cv = (stdev / mean * 100)
              
              sorted_durations = sorted(durations)
              p50 = sorted_durations[len(sorted_durations)//2]
              p95 = sorted_durations[int(len(sorted_durations)*0.95)]
              p99 = sorted_durations[int(len(sorted_durations)*0.99)] if len(sorted_durations) > 2 else sorted_durations[-1]
              
              print(f"### Statistical Summary (n={len(durations)})")
              print(f"- **Mean**: {mean:.0f}ms")
              print(f"- **Median (P50)**: {p50:.0f}ms")
              print(f"- **Std Dev**: {stdev:.0f}ms")
              print(f"- **Coefficient of Variation (CV)**: {cv:.1f}%")
              print(f"- **P95**: {p95:.0f}ms")
              print(f"- **P99**: {p99:.0f}ms")
              print(f"- **Min**: {min(durations):.0f}ms")
              print(f"- **Max**: {max(durations):.0f}ms")
              print(f"- **Range**: {max(durations) - min(durations):.0f}ms")
              print()
              print("### Performance Consistency Rating")
              
              if cv < 10:
                  print("âœ… **EXCELLENT** - CV < 10%")
                  print("- Highly predictable performance")
                  print("- Reliable for SLA commitments")
                  print("- Accurate capacity planning possible")
              elif cv < 20:
                  print("âš ï¸ **MODERATE** - CV 10-20%")
                  print("- Acceptable variance")
                  print("- Some unpredictability")
                  print("- Consider investigating outliers")
              else:
                  print("âŒ **HIGH VARIANCE** - CV > 20%")
                  print("- Unpredictable performance")
                  print("- Difficult to plan capacity")
                  print("- May impact developer experience")
              
              print()
              print("### What This Means")
              print(f"In {len(durations)} identical runs:")
              print(f"- Performance varied by {max(durations) - min(durations):.0f}ms ({(max(durations) - min(durations))/mean*100:.1f}%)")
              print(f"- 95% of runs completed within {p95:.0f}ms")
              print(f"- Average deviation from mean: {stdev:.0f}ms")
              print()
              print("### Why CV Matters")
              print("- **Low CV (5-10%)**: Predictable, reliable, good for planning")
              print("- **High CV (>20%)**: Unpredictable, 'noisy neighbors', poor planning")
              print()
              print("Standard GitHub Actions often shows 20-25% CV.")
              print("Blacksmith typically achieves 5-10% CV.")
              print()
              print("### The Business Impact")
              print(f"With {cv:.1f}% CV, you can:")
              if cv < 10:
                  print("- Commit to aggressive SLAs with confidence")
                  print("- Accurately forecast Q4 compute costs")
                  print("- Plan sprint capacity with precision")
              else:
                  print("- May need to over-provision by 30%+ for safety")
                  print("- Difficult to commit to tight SLAs")
                  print("- Build time estimates are unreliable")
          PYTHON
